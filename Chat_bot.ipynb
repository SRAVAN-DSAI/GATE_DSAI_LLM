{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":12640659,"sourceType":"datasetVersion","datasetId":7987370}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":" # This Python 3 environment comes with many helpful analytics libraries installed\n # It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n # For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":" =============================================================================\n STEP 1: ENVIRONMENT SETUP AND LIBRARY INSTALLATION\n =============================================================================\n\n In this step, we install all the necessary Python packages. Kaggle environments\n are ephemeral, so these installations are required for each session. We use\n quiet flags (-q) to keep the output clean.\n\n Key Libraries:\n - langchain: The core framework for building LLM applications.\n - transformers: Provides access to Hugging Face models and pipelines.\n - faiss-cpu: A library for efficient similarity search (our vector store).\n - sentence-transformers: Used for generating high-quality text embeddings.\n - pypdf: A robust library for extracting text from PDF documents.\n - accelerate: Required for efficiently loading and running large models on GPUs.\n\n =============================================================================","metadata":{}},{"cell_type":"code","source":"pip uninstall sentence-transformers--y","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"STEP 1: Installing required libraries...\")\n# This consolidated and upgraded installation command helps pip resolve\n# dependencies more effectively in the complex Kaggle environment.\n!pip install -q --upgrade --no-cache-dir \\\n    \"transformers>=4.41.0\" \\\n    \"accelerate>=0.30.0\" \\\n    torch \\\n    \"langchain>=0.2.0\" \\\n    \"langchain-community[pdf]>=0.2.0\" \\\n    faiss-cpu \\\n    sentence-transformers \\\n    pypdf\n\nprint(\"Library installation complete.\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T12:03:26.629268Z","iopub.execute_input":"2025-08-01T12:03:26.629829Z","iopub.status.idle":"2025-08-01T12:03:31.568098Z","shell.execute_reply.started":"2025-08-01T12:03:26.629795Z","shell.execute_reply":"2025-08-01T12:03:31.566610Z"}},"outputs":[{"name":"stdout","text":"STEP 1: Installing required libraries...\n\u001b[33mWARNING: langchain-community 0.3.27 does not provide the extra 'pdf'\u001b[0m\u001b[33m\n\u001b[0mLibrary installation complete.\n\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":" =============================================================================\n STEP 2: LOAD THE PRE-BUILT VECTOR INDEX\n =============================================================================\n\n Instead of processing PDFs, we directly load the saved FAISS index.\n This is significantly faster.\n\n=============================================================================","metadata":{}},{"cell_type":"code","source":"import os\nfrom langchain_community.vectorstores import FAISS\nfrom langchain_community.embeddings import HuggingFaceEmbeddings\n\nprint(\"STEP 2: Loading the pre-built vector index...\")\n\n# Define the path to the saved index\nindex_path = \"/kaggle/input/gate-dsai-llm/my_vector_index\"\n\n# Check if the index exists before trying to load\nif not os.path.exists(index_path):\n    raise FileNotFoundError(\n        f\"Index not found at '{index_path}'. \"\n        \"Please run the 'build_index.ipynb' notebook first to create it.\"\n    )\n\n# Initialize the same embedding model used to create the index\nembedding_model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\nembeddings = HuggingFaceEmbeddings(model_name=embedding_model_name)\n\n# Load the vector store from disk\nvector_store = FAISS.load_local(index_path, embeddings, allow_dangerous_deserialization=True)\n\nprint(\"Vector index loaded successfully.\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T12:03:35.558136Z","iopub.execute_input":"2025-08-01T12:03:35.558537Z","iopub.status.idle":"2025-08-01T12:03:35.624710Z","shell.execute_reply.started":"2025-08-01T12:03:35.558500Z","shell.execute_reply":"2025-08-01T12:03:35.623523Z"}},"outputs":[{"name":"stdout","text":"STEP 2: Loading the pre-built vector index...\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_community/embeddings/huggingface.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m             \u001b[0;32mimport\u001b[0m \u001b[0msentence_transformers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sentence_transformers/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m )\n\u001b[0;32m---> 15\u001b[0;31m from sentence_transformers.cross_encoder import (\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mCrossEncoder\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sentence_transformers/cross_encoder/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mCrossEncoder\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCrossEncoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmodel_card\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCrossEncoderModelCardData\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sentence_transformers/cross_encoder/CrossEncoder.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautonotebook\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrange\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m from transformers import (\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0mAutoConfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mImportError\u001b[0m: cannot import name 'PreTrainedModel' from 'transformers' (/usr/local/lib/python3.11/dist-packages/transformers/__init__.py)","\nThe above exception was the direct cause of the following exception:\n","\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_295/2268487606.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# Initialize the same embedding model used to create the index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0membedding_model_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"sentence-transformers/all-MiniLM-L6-v2\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHuggingFaceEmbeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0membedding_model_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# Load the vector store from disk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/_api/deprecation.py\u001b[0m in \u001b[0;36mwarn_if_direct_instance\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    220\u001b[0m                         \u001b[0mwarned\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m                         \u001b[0memit_warning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m                 obj.__init__ = functools.wraps(obj.__init__)(  # type: ignore[misc]\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_community/embeddings/huggingface.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m             raise ImportError(\n\u001b[0m\u001b[1;32m     88\u001b[0m                 \u001b[0;34m\"Could not import sentence_transformers python package. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m                 \u001b[0;34m\"Please install it with `pip install sentence-transformers`.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mImportError\u001b[0m: Could not import sentence_transformers python package. Please install it with `pip install sentence-transformers`."],"ename":"ImportError","evalue":"Could not import sentence_transformers python package. Please install it with `pip install sentence-transformers`.","output_type":"error"}],"execution_count":3},{"cell_type":"markdown","source":" =============================================================================\n STEP 3: LARGE LANGUAGE MODEL (LLM) INITIALIZATION\n =============================================================================\n\n We now load our generative AI model. We are using Microsoft's Phi-3-mini,\n a state-of-the-art small language model known for high-quality reasoning\n and generation. We use the 'transformers' library to create a pipeline,\n which is then wrapped by LangChain for seamless integration.\n\n `device_map=\"auto\"` intelligently utilizes the available Kaggle GPU.\n\n=============================================================================","metadata":{}},{"cell_type":"code","source":"from langchain_community.llms import HuggingFacePipeline\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\nimport torch\n\nprint(\"STEP 3: Initializing the Large Language Model (Phi-3-mini)...\")\nprint(\"(This may take a few minutes as the model is loaded into the GPU)\")\n\n# Define the model ID from Hugging Face\nmodel_id = \"microsoft/Phi-3-mini-4k-instruct\"\n\n# Load the model and its tokenizer.\n# `trust_remote_code=True` is required for this model architecture.\ntokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_id,\n    trust_remote_code=True,\n    torch_dtype=\"auto\",\n    device_map=\"auto\" # This automatically uses the GPU\n)\n\n# Create a text-generation pipeline.\npipe = pipeline(\n    \"text-generation\",\n    model=model,\n    tokenizer=tokenizer,\n    max_new_tokens=512,  # Max length of the generated answer\n    return_full_text=False # We only want the generated response\n)\n\n# Wrap the pipeline in a LangChain-compatible object.\nllm = HuggingFacePipeline(pipeline=pipe)\n\nprint(\"LLM initialized successfully.\\n\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":" =============================================================================\n STEP 4: CREATING THE QUESTION-ANSWERING (QA) CHAIN\n =============================================================================\n\n This step ties everything together. We create a `RetrievalQA` chain, which\n orchestrates the entire RAG process. We use the \"refine\" chain type, which\n is robust for detailed questions. It iterates through the retrieved documents,\n progressively building and refining the answer, which helps generate more\n comprehensive and accurate responses.\n\n =============================================================================","metadata":{}},{"cell_type":"code","source":"from langchain.chains import RetrievalQA\n\nprint(\"STEP 4: Creating the RetrievalQA chain...\")\n\n# The \"refine\" chain type is ideal for generating detailed answers from\n# multiple document sources.\nqa_chain = RetrievalQA.from_chain_type(\n    llm=llm,\n    chain_type=\"refine\",\n    retriever=vector_store.as_retriever(),\n    return_source_documents=True,\n)\n\nprint(\"QA chain is ready.\\n\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":" =============================================================================\n STEP 5: INTERACTIVE CHATBOT INTERFACE\n =============================================================================\n\n This final block runs a simple command-line interface, allowing you to\n interact with your custom GATE DSAI study bot.\n\n =============================================================================","metadata":{}},{"cell_type":"code","source":"print(\"--- GATE DSAI Study Bot Initialized ---\")\nprint(\"You can now ask questions about your documents.\")\nprint(\"Type 'quit' or 'exit' to end the session.\")\n\nwhile True:\n    try:\n        query = input(\"\\nYour Question: \")\n        if query.lower() in [\"quit\", \"exit\"]:\n            print(\"Session ended. Goodbye!\")\n            break\n        if not query.strip():\n            continue\n\n        # Invoke the QA chain with the user's query\n        result = qa_chain.invoke({\"query\": query})\n\n        # Display the results\n        print(\"\\nAnswer:\")\n        print(result[\"result\"])\n        print(\"\\nSources Used:\")\n        # Display the source documents for verification\n        for doc in result[\"source_documents\"]:\n            print(f\"  - {doc.metadata.get('source', 'Unknown source')}\")\n\n    except Exception as e:\n        print(f\"\\nAn error occurred: {e}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}