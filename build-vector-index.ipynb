{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":12639824,"sourceType":"datasetVersion","datasetId":7987370}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":" # This Python 3 environment comes with many helpful analytics libraries installed\n # It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n # For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-08-01T10:15:52.808911Z","iopub.execute_input":"2025-08-01T10:15:52.809635Z","iopub.status.idle":"2025-08-01T10:15:53.198837Z","shell.execute_reply.started":"2025-08-01T10:15:52.809605Z","shell.execute_reply":"2025-08-01T10:15:53.198015Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/gate-dsai-llm/build_index.ipynb\n/kaggle/input/gate-dsai-llm/GATE2024_DA_Sample_Paper.pdf\n/kaggle/input/gate-dsai-llm/README.md\n/kaggle/input/gate-dsai-llm/Chat_bot.ipynb\n/kaggle/input/gate-dsai-llm/GATE_DA_2025_Question_Paper.pdf\n/kaggle/input/gate-dsai-llm/GATE_DA_2025_Syllabus.pdf\n/kaggle/input/gate-dsai-llm/Machine-Learning/ML-A-Probabilistic-Perspective-Murphy.pdf\n/kaggle/input/gate-dsai-llm/Machine-Learning/UPenn-CIS520-Midterm2019-Solutions.pdf\n/kaggle/input/gate-dsai-llm/Machine-Learning/CMU-ML-Notes.pdf\n/kaggle/input/gate-dsai-llm/Machine-Learning/CS229-Stanford-Lecture-Notes-Repo-Copy.pdf\n/kaggle/input/gate-dsai-llm/Machine-Learning/ML_super_cheatsheet.pdf\n/kaggle/input/gate-dsai-llm/Machine-Learning/UPenn-CIS520-Final2017-Solutions.pdf\n/kaggle/input/gate-dsai-llm/Machine-Learning/UPenn-CIS520-Final2018-Solutions.pdf\n/kaggle/input/gate-dsai-llm/Machine-Learning/UPenn-CIS520-Midterm2022-Solutions.pdf\n/kaggle/input/gate-dsai-llm/Machine-Learning/UPenn-CIS520-Midterm2018-Solutions.pdf\n/kaggle/input/gate-dsai-llm/Machine-Learning/CS229-Stanford-Lecture-Notes.pdf\n/kaggle/input/gate-dsai-llm/Machine-Learning/Learning-From-Data-Abu-Mostafa.pdf\n/kaggle/input/gate-dsai-llm/Machine-Learning/UPenn-CIS520-Final2019-Solutions.pdf\n/kaggle/input/gate-dsai-llm/Artificial-Intelligence/AI_All_cheat_sheet.pdf\n/kaggle/input/gate-dsai-llm/Artificial-Intelligence/handout3.pdf\n/kaggle/input/gate-dsai-llm/Artificial-Intelligence/handout2.pdf\n/kaggle/input/gate-dsai-llm/Artificial-Intelligence/handout4.pdf\n/kaggle/input/gate-dsai-llm/Artificial-Intelligence/handout1.pdf\n/kaggle/input/gate-dsai-llm/Probability-Statistics/Probability-Review-IIT-D.pdf\n/kaggle/input/gate-dsai-llm/Probability-Statistics/Intro-to-Probability-Blitzstein-Hwang.pdf\n/kaggle/input/gate-dsai-llm/Probability-Statistics/probability_cheatsheet.pdf\n/kaggle/input/gate-dsai-llm/Probability-Statistics/All-of-Statistics-Wasserman.pdf\n/kaggle/input/gate-dsai-llm/Probability-Statistics/Intro-to-Probability-Pishro-Nik.pdf\n/kaggle/input/gate-dsai-llm/Probability-Statistics/Multivariate-Gaussians-Review-IIT-D.pdf\n/kaggle/input/gate-dsai-llm/Probability-Statistics/A-First-Course-in-Probability-Sheldon-Ross.pdf\n/kaggle/input/gate-dsai-llm/Database-Management/Fundamentals-of-Database-Systems-Elmasri-Navathe.pdf\n/kaggle/input/gate-dsai-llm/Database-Management/Intro-to-Data-Mining-Tan.pdf\n/kaggle/input/gate-dsai-llm/Database-Management/Indexing-File-Organization-GWU.pdf\n/kaggle/input/gate-dsai-llm/Database-Management/Data-Mining-Concepts-and-Techniques-Han.pdf\n/kaggle/input/gate-dsai-llm/my_vector_index/index.faiss\n/kaggle/input/gate-dsai-llm/my_vector_index/index.pkl\n/kaggle/input/gate-dsai-llm/Programming-and-Algorithms/Python-Programming-Berkeley.pdf\n/kaggle/input/gate-dsai-llm/Programming-and-Algorithms/Data-Structures-Algorithms-in-Python-Sample.pdf\n/kaggle/input/gate-dsai-llm/Programming-and-Algorithms/Data-Structures-and-Algorithms-Princeton.pdf\n/kaggle/input/gate-dsai-llm/Programming-and-Algorithms/DS-Algo-Cheatsheet-WMich.pdf\n/kaggle/input/gate-dsai-llm/Programming-and-Algorithms/Algorithms-Notes-UWA.pdf\n/kaggle/input/gate-dsai-llm/Programming-and-Algorithms/Algorithm-Design-Kleinberg-Tardos.pdf\n/kaggle/input/gate-dsai-llm/Programming-and-Algorithms/Big-O-Refresher-MIT.pdf\n/kaggle/input/gate-dsai-llm/Programming-and-Algorithms/Data-Structures-Stanford.pdf\n/kaggle/input/gate-dsai-llm/Linear-Algebra/Linear-Algebra-Review-for-ML-IIT-D.pdf\n/kaggle/input/gate-dsai-llm/Linear-Algebra/MIT18_06S10ZoomNotes-Gilbert-Strang.pdf\n/kaggle/input/gate-dsai-llm/Linear-Algebra/Linear-Algebra-Done-Right-Sheldon-Axler.pdf\n/kaggle/input/gate-dsai-llm/Linear-Algebra/refresher-algebra-calculus.pdf\n/kaggle/input/gate-dsai-llm/Calculus-and-Optimization/Convex-Optimization-Boyd.pdf\n/kaggle/input/gate-dsai-llm/Calculus-and-Optimization/Calculus-Online-Textbook-Gilbert-Strang.pdf\n/kaggle/input/gate-dsai-llm/Calculus-and-Optimization/NPTEL-Math-for-ML-Assignment-Week5.pdf\n/kaggle/input/gate-dsai-llm/Calculus-and-Optimization/Optimization-Notes-IIT-D.pdf\n/kaggle/input/gate-dsai-llm/Calculus-and-Optimization/calculus_cheat_sheet_all.pdf\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":" =============================================================================\n STEP 1: ENVIRONMENT SETUP AND LIBRARY INSTALLATION\n =============================================================================\n\n In this step, we install all the necessary Python packages. Kaggle environments\n are ephemeral, so these installations are required for each session. We use\n quiet flags (-q) to keep the output clean.\n\n Key Libraries:\n - langchain: The core framework for building LLM applications.\n - transformers: Provides access to Hugging Face models and pipelines.\n - faiss-cpu: A library for efficient similarity search (our vector store).\n - sentence-transformers: Used for generating high-quality text embeddings.\n - pypdf: A robust library for extracting text from PDF documents.\n - accelerate: Required for efficiently loading and running large models on GPUs.\n\n =============================================================================","metadata":{}},{"cell_type":"code","source":"print(\"STEP 1: Installing required libraries...\")\n!pip install -q langchain \"langchain-community[pdf]\"\n!pip install -q faiss-cpu sentence-transformers\n!pip install -q transformers torch accelerate\nprint(\"Library installation complete.\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T10:16:20.321598Z","iopub.execute_input":"2025-08-01T10:16:20.322032Z","iopub.status.idle":"2025-08-01T10:18:06.134474Z","shell.execute_reply.started":"2025-08-01T10:16:20.322007Z","shell.execute_reply":"2025-08-01T10:18:06.133058Z"}},"outputs":[{"name":"stdout","text":"STEP 1: Installing required libraries...\n\u001b[33mWARNING: langchain-community 0.3.27 does not provide the extra 'pdf'\u001b[0m\u001b[33m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.2/45.2 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m34.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.5/65.5 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.8.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ndatasets 3.6.0 requires fsspec[http]<=2025.3.0,>=2023.1.0, but you have fsspec 2025.5.1 which is incompatible.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\npandas-gbq 0.29.1 requires google-api-core<3.0.0,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\nthinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\nplotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.2 which is incompatible.\ndataproc-spark-connect 0.7.5 requires google-api-core>=2.19, but you have google-api-core 1.34.1 which is incompatible.\nbigframes 2.8.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\nbigframes 2.8.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\nmlxtend 0.23.4 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.3/31.3 MB\u001b[0m \u001b[31m58.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m102.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m80.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m44.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m32.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m83.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hLibrary installation complete.\n\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":" =============================================================================\n STEP 2: DATA LOADING AND DOCUMENT PROCESSING\n =============================================================================\n\n Here, we define the path to our custom dataset on Kaggle. We then\n recursively walk through the directory, identify all PDF files, and use\n LangChain's PyPDFLoader to load and split the documents into manageable\n chunks. This chunking is essential for the retrieval process.\n\n =============================================================================","metadata":{}},{"cell_type":"code","source":"import os\nfrom langchain_community.document_loaders import PyPDFLoader\n\ndocs_path = \"/kaggle/input/gate-dsai-llm\"\n\nprint(f\"STEP 2: Loading documents from '{docs_path}'...\")\n\nall_docs = []\n# os.walk is a Python generator that explores a directory tree.\nfor root, _, files in os.walk(docs_path):\n    for file in files:\n        if file.endswith(\".pdf\"):\n            try:\n                file_path = os.path.join(root, file)\n                loader = PyPDFLoader(file_path)\n                # load_and_split() is more efficient than loading then splitting.\n                loaded_docs = loader.load_and_split()\n                all_docs.extend(loaded_docs)\n                print(f\"  -> Successfully loaded and split: {file}\")\n            except Exception as e:\n                print(f\"  -> ERROR loading {file}: {e}\")\n\nif not all_docs:\n    raise ValueError(\"No documents were loaded. Check the dataset path and ensure it contains PDF files.\")\n\nprint(f\"\\nTotal document chunks loaded: {len(all_docs)}\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T10:19:28.466826Z","iopub.execute_input":"2025-08-01T10:19:28.467241Z","iopub.status.idle":"2025-08-01T10:28:37.618632Z","shell.execute_reply.started":"2025-08-01T10:19:28.467203Z","shell.execute_reply":"2025-08-01T10:28:37.617542Z"}},"outputs":[{"name":"stdout","text":"STEP 2: Loading documents from '/kaggle/input/gate-dsai-llm'...\n  -> Successfully loaded and split: GATE2024_DA_Sample_Paper.pdf\n  -> Successfully loaded and split: GATE_DA_2025_Question_Paper.pdf\n  -> Successfully loaded and split: GATE_DA_2025_Syllabus.pdf\n  -> Successfully loaded and split: ML-A-Probabilistic-Perspective-Murphy.pdf\n  -> Successfully loaded and split: UPenn-CIS520-Midterm2019-Solutions.pdf\n  -> Successfully loaded and split: CMU-ML-Notes.pdf\n  -> Successfully loaded and split: CS229-Stanford-Lecture-Notes-Repo-Copy.pdf\n  -> Successfully loaded and split: ML_super_cheatsheet.pdf\n  -> Successfully loaded and split: UPenn-CIS520-Final2017-Solutions.pdf\n  -> Successfully loaded and split: UPenn-CIS520-Final2018-Solutions.pdf\n  -> Successfully loaded and split: UPenn-CIS520-Midterm2022-Solutions.pdf\n  -> Successfully loaded and split: UPenn-CIS520-Midterm2018-Solutions.pdf\n  -> Successfully loaded and split: CS229-Stanford-Lecture-Notes.pdf\n  -> Successfully loaded and split: Learning-From-Data-Abu-Mostafa.pdf\n  -> Successfully loaded and split: UPenn-CIS520-Final2019-Solutions.pdf\n  -> Successfully loaded and split: AI_All_cheat_sheet.pdf\n  -> Successfully loaded and split: handout3.pdf\n  -> Successfully loaded and split: handout2.pdf\n  -> Successfully loaded and split: handout4.pdf\n  -> Successfully loaded and split: handout1.pdf\n  -> Successfully loaded and split: Probability-Review-IIT-D.pdf\n  -> Successfully loaded and split: Intro-to-Probability-Blitzstein-Hwang.pdf\n  -> Successfully loaded and split: probability_cheatsheet.pdf\n  -> Successfully loaded and split: All-of-Statistics-Wasserman.pdf\n  -> Successfully loaded and split: Intro-to-Probability-Pishro-Nik.pdf\n  -> Successfully loaded and split: Multivariate-Gaussians-Review-IIT-D.pdf\n  -> Successfully loaded and split: A-First-Course-in-Probability-Sheldon-Ross.pdf\n  -> Successfully loaded and split: Fundamentals-of-Database-Systems-Elmasri-Navathe.pdf\n  -> Successfully loaded and split: Intro-to-Data-Mining-Tan.pdf\n  -> Successfully loaded and split: Indexing-File-Organization-GWU.pdf\n  -> Successfully loaded and split: Data-Mining-Concepts-and-Techniques-Han.pdf\n  -> Successfully loaded and split: Python-Programming-Berkeley.pdf\n  -> Successfully loaded and split: Data-Structures-Algorithms-in-Python-Sample.pdf\n  -> Successfully loaded and split: Data-Structures-and-Algorithms-Princeton.pdf\n  -> Successfully loaded and split: DS-Algo-Cheatsheet-WMich.pdf\n  -> Successfully loaded and split: Algorithms-Notes-UWA.pdf\n  -> Successfully loaded and split: Algorithm-Design-Kleinberg-Tardos.pdf\n  -> Successfully loaded and split: Big-O-Refresher-MIT.pdf\n  -> Successfully loaded and split: Data-Structures-Stanford.pdf\n  -> Successfully loaded and split: Linear-Algebra-Review-for-ML-IIT-D.pdf\n  -> Successfully loaded and split: MIT18_06S10ZoomNotes-Gilbert-Strang.pdf\n  -> Successfully loaded and split: Linear-Algebra-Done-Right-Sheldon-Axler.pdf\n  -> Successfully loaded and split: refresher-algebra-calculus.pdf\n  -> Successfully loaded and split: Convex-Optimization-Boyd.pdf\n  -> Successfully loaded and split: Calculus-Online-Textbook-Gilbert-Strang.pdf\n  -> Successfully loaded and split: NPTEL-Math-for-ML-Assignment-Week5.pdf\n  -> Successfully loaded and split: Optimization-Notes-IIT-D.pdf\n  -> Successfully loaded and split: calculus_cheat_sheet_all.pdf\n\nTotal document chunks loaded: 11473\n\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":" =============================================================================\n STEP 3: VECTOR EMBEDDING AND INDEX CREATION\n =============================================================================\n\n This is the core of the \"Retrieval\" part of RAG. We convert our text chunks\n into numerical vectors (embeddings) using a sentence-transformer model.\n These vectors are then stored in a FAISS index, which allows for extremely\n fast similarity searches. When a user asks a question, we will embed their\n question and use this index to find the most semantically relevant chunks.\n\n =============================================================================","metadata":{}},{"cell_type":"code","source":"from langchain_community.vectorstores import FAISS\nfrom langchain_community.embeddings import HuggingFaceEmbeddings\n\nprint(\"STEP 3: Creating vector embeddings and FAISS index...\")\n\n# Initialize the embedding model. 'all-MiniLM-L6-v2' is a great, lightweight\n# model that provides excellent performance for semantic search.\nembedding_model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\nembeddings = HuggingFaceEmbeddings(model_name=embedding_model_name)\n\n# Create the FAISS vector store from our document chunks and embeddings.\n# This process can take a few minutes depending on the number of documents.\nvector_store = FAISS.from_documents(all_docs, embeddings)\n\nprint(\"Vector index created successfully.\\n\")\noutput_path = \"/kaggle/working/my_vector_index\"\nvector_store.save_local(output_path)\n\nprint(f\"STEP 4: Index successfully saved to '{output_path}'\")\nprint(\"\\n--- BUILD PROCESS COMPLETE ---\")\nprint(\"You can now run the 'chatbot.ipynb' notebook.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T10:37:23.720767Z","iopub.execute_input":"2025-08-01T10:37:23.721495Z","iopub.status.idle":"2025-08-01T10:49:06.074335Z","shell.execute_reply.started":"2025-08-01T10:37:23.721466Z","shell.execute_reply":"2025-08-01T10:49:06.073402Z"}},"outputs":[{"name":"stdout","text":"STEP 3: Creating vector embeddings and FAISS index...\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_36/3473037497.py:9: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n  embeddings = HuggingFaceEmbeddings(model_name=embedding_model_name)\n2025-08-01 10:37:42.650595: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1754044663.008009      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1754044663.096525      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"90c71931917a4fbfbb5eaa00ca91faf4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"595f401ed0c34b75a6fa6c9df5797a88"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6f07f485e17c4bb7a753717dedc715fb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"834c593b3bd74615b34e0eb8dc11d397"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5ba99112f9c64f4e9b805b128a190b1e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aeabe5ed9c7d4c43a6667d68186bf682"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9080164a2fe146a4ac36f2a80c1cb4ae"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a432be455fa3411db53862f3416829f7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f92ae31526c3476fbac8564555e16dba"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"82406b2392424b2f934b11a9072ee310"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c68fc2488ada4fef9d0f67b275e8291a"}},"metadata":{}},{"name":"stdout","text":"Vector index created successfully.\n\nSTEP 4: Index successfully saved to '/kaggle/working/my_vector_index'\n\n--- BUILD PROCESS COMPLETE ---\nYou can now run the 'chatbot.ipynb' notebook.\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":" =============================================================================\n STEP 4: LARGE LANGUAGE MODEL (LLM) INITIALIZATION\n =============================================================================\n\n We now load our generative AI model. We are using Microsoft's Phi-3-mini,\n a state-of-the-art small language model known for high-quality reasoning\n and generation. We use the 'transformers' library to create a pipeline,\n which is then wrapped by LangChain for seamless integration.\n\n `device_map=\"auto\"` intelligently utilizes the available Kaggle GPU.\n\n=============================================================================","metadata":{}},{"cell_type":"code","source":"from langchain_community.llms import HuggingFacePipeline\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\nimport torch\n\nprint(\"STEP 4: Initializing the Large Language Model (Phi-3-mini)...\")\nprint(\"(This may take a few minutes as the model is loaded into the GPU)\")\n\n# Define the model ID from Hugging Face\nmodel_id = \"microsoft/Phi-3-mini-4k-instruct\"\n\n# Load the model and its tokenizer.\n# `trust_remote_code=True` is required for this model architecture.\ntokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_id,\n    trust_remote_code=True,\n    torch_dtype=\"auto\",\n    device_map=\"auto\" # This automatically uses the GPU\n)\n\n# Create a text-generation pipeline.\npipe = pipeline(\n    \"text-generation\",\n    model=model,\n    tokenizer=tokenizer,\n    max_new_tokens=512,  # Max length of the generated answer\n    return_full_text=False # We only want the generated response\n)\n\n# Wrap the pipeline in a LangChain-compatible object.\nllm = HuggingFacePipeline(pipeline=pipe)\n\nprint(\"LLM initialized successfully.\\n\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":" =============================================================================\n STEP 5: CREATING THE QUESTION-ANSWERING (QA) CHAIN\n =============================================================================\n\n This step ties everything together. We create a `RetrievalQA` chain, which\n orchestrates the entire RAG process. We use the \"refine\" chain type, which\n is robust for detailed questions. It iterates through the retrieved documents,\n progressively building and refining the answer, which helps generate more\n comprehensive and accurate responses.\n\n =============================================================================","metadata":{}},{"cell_type":"code","source":"from langchain.chains import RetrievalQA\n\nprint(\"STEP 5: Creating the RetrievalQA chain...\")\n\n# The \"refine\" chain type is ideal for generating detailed answers from\n# multiple document sources.\nqa_chain = RetrievalQA.from_chain_type(\n    llm=llm,\n    chain_type=\"refine\",\n    retriever=vector_store.as_retriever(),\n    return_source_documents=True,\n)\n\nprint(\"QA chain is ready.\\n\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":" =============================================================================\n STEP 6: INTERACTIVE CHATBOT INTERFACE\n =============================================================================\n\n This final block runs a simple command-line interface, allowing you to\n interact with your custom GATE DSAI study bot.\n\n =============================================================================","metadata":{}},{"cell_type":"code","source":"print(\"--- GATE DSAI Study Bot Initialized ---\")\nprint(\"You can now ask questions about your documents.\")\nprint(\"Type 'quit' or 'exit' to end the session.\")\n\nwhile True:\n    try:\n        query = input(\"\\nYour Question: \")\n        if query.lower() in [\"quit\", \"exit\"]:\n            print(\"Session ended. Goodbye!\")\n            break\n        if not query.strip():\n            continue\n\n        # Invoke the QA chain with the user's query\n        result = qa_chain.invoke({\"query\": query})\n\n        # Display the results\n        print(\"\\nAnswer:\")\n        print(result[\"result\"])\n        print(\"\\nSources Used:\")\n        # Display the source documents for verification\n        for doc in result[\"source_documents\"]:\n            print(f\"  - {doc.metadata.get('source', 'Unknown source')}\")\n\n    except Exception as e:\n        print(f\"\\nAn error occurred: {e}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}